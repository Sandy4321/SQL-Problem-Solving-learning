#COMPONENTS OF HADOOP ECOSYSTEM

DATA INGESTION = SQOOP , FLUEME
DATA STORAGE= S3, HDFS
DATA PROCESSING= HADOOP MAPREDUCE(YARN),APACHE SPARK(CLUSTER MGMT)
DATA ANALYSIS= HIVE,PIG,IMPALA
DATA EXPLORATION= CLOUDERA SEARCH,HUE,OZZIE
DATA VISUALIZATION= TABLEAU

HDFS=PROVIDE storage layer for hadoop,suitable for distributed storage and processing,streaming access to 
file system data,provide file permission and authentication,provides a cli to interact with  HDFS.

HBASE= STORE DATA IN HDFS,HBASE IS  A NOSQL OR NON RELATIONAL DATABASE,MAINLY USED WHEN YOU NEED RANDOM
real time,read/write access to your big data ,provide support to high volume of data and high throughput

SQOOP : A TOOL TO DESIGNED TO TRANSFER DATA BETWEEN HADOOP AND RELATIONAL DATABASE SERVER
IT IS  used to import data from RELATIONAL DATABASEs such as oracle and mysql to hdfs and export data from hdfs to relational databases

FLUEME=if you want to ingest event data such as streaming data ,ensor data  or log files you can use FLUME.
IDEALLY SUITED for event data from multiple system,a distributed service for ingesting streaming data

SPARK= open source cluster computing framework,provide 100 times faster performance than mapreduce,
support machine learning and batch processing

SPARK HAS FOLLOWING MAIN COMPONENTS

SPARK CORE AND RESILLIENT DISTRIBUTED DATASETS(RDDs),SPARK SQL,SPARK STREAMING ,MLIib,GRAPHX

HADOOP=THE ORIGINAL hadoop processing engine which is primarily java based,based on the map and reduce programming model
,an extensive and fault tolerance framework,commonly used

PIG=AN open source dataflow system,used mainly for analysis, converts pig scripts to map reduce code
,ana alternative to writing map reduce code,best for ad hoc queries such as join and filter

IMPALA=HIGH PERFORMANCE sql engine WHICH RUNS ON HADOP CLUSTER, IDEAL FOR INTERACTIVE ANALYSIS,
,very low latency-measured in miliseconds,support a dialect of sql(impala sql)

HIVE= SIMILAR TO IMPALA,BEST FOR DATA PROCESSING AND ETL,execute querries using mapreduce,

CLOUDERA SEARCH= one of cloudera near real time access product,enable non tech users to search and explore data stored in or ingested into hadoop and hbase
,a fully integrated data processing platform,eliminate the need to move large datasets across infrastructures to address business tasks

OOZIE= is a workflow and coordination system used to manage the hadoop jobs

HUE(Hadoop user experience)=open source web interface for analyzing data with hadoop,it provides SQL editor for hive,impala,mysql,oracle,postgreSQL,spark SQL,AND SOLARSQL
text interface for exploring data.

commercial HADOOP DISTRIBUTION= cloudera,hortonworks,mapr,AMAZON EMR,AZURE HDInsight

#BIG DATA PROCESSING
1.ingest      = flume,oozie,apache kafka
2.processing  = hadoop,hbase,hadoop mapreduce,spark
3.analyze     = hive,impala,pig
4.access      = hue,cloudera

#HDFS & YARN
hdfs IS A DISTRIBUTED FILE SYSTEM THAT PROVIDES ACCESS TO DATA ACROSS HADOOP CLUSTER
,CLUSTER IS A GROUP OF COMPUTERS,HDFS MANAGES AND SUPPORTS ANALYSIS OF very large volume of big data

traditional system challenges = cost ,speed ,reliability,

HDFS RESOLVES ALL 3 MAJOR ISSUES
HDFS COPIES DATA MULTIPLE TIMES,HADOOP CLUSTER READ OR WRITE MORE THAN ONE TB DATA IN A SECOND,ZERO LICENSING AND SUPPORT COST

FOR EXample a person donates book to library,librarian decides to arrange the books on a smaal rack,also he distributes multiple copies of each book on other racks
for easy access.

HDFS HAS 128MB -LARGE BLOCK OF DATA ,WHILE TRADITIONAL FILE SYSTEM HAS 5 SMALL BYTES OF DATA,
ACCESS TO LARGE DATA IN HDFS READS HUGE DATA SEQUENTIALLY IN A SINGLE OPERATION,WHILE IN traditional file system suffers form disk i/o problem
coz of multiple seek operation

characteristics of HDFS=built for large datasets,support for hetrogenious clusters,rack-aware,scalable,fault tolerant

HDFS ARCHITECTURE-MASTER-SLAVE 
TWO OR MORE SEPRATE MACHINE ARE CONFIGURED AS NAMENODE,ONLY 1 IS ACTIVE AT A  time,all other name nodes are at standby state
,the active namenode is responsible for client opeartion in the cluster,stanby  namenodes simlply acts as a slave maintaining the state to provide 
fast failover if necessary, a datanodes serves read or write request,it also create ,deletes and replicates blocks
based on the instruction from the name node,stand by namenode and active namenode keep in sync with each other
through share edit logs or metadata,the active node update the edit logs about storage, this is done with namespace modification
like block modification,standby nodes reads the changes made by dit logs and apply it to its on namespace in a consistent manner,
in the event of failover ,the standby will ensure that it has read all the edit before promoting itself to active state, this is manual failover perform by admin
for automatic failover we nee to utilize service of apache zookeeper which maintains small amount of coordination data,zookeper maintains an open session with active namenode by periodically pinging it
with health check command,if the node has crashed or frozen or otherwise entered in unhealthy state, the health monitor mark it as unhealthy
and elect a new namenode

NAMENODE SERVER- CORE COMPONENT OF AN HDFS CLUSTER,
datanode,is a multiple instance server ,the datanode server is responsible for storing and maintaing data blocks

zooekeeper allows distributed process to coordinate with  each othe through a shared hirarchical namespace
zookeeper implementation is simple and replicated which puts a premium on high performance,high availability, and strictly ordered access

namespace =allow user data to be stored in files,follows a hirarchical file systems with directories and files,support operation such as create ,remove,rename,move

namenode=it maintains two persistent files, atransaction log called an edit log, a namespace image called  an fsimage
 
SQOOP =SMOOTH IMPORT /EXPORT OF DATA FROM STRUCTURED DATABASEs,CAN BE USED IN ONLINE MARKETING
SQOOP PROCESSING=runs in hadoop cluster,it imports data from rdb or nosql db to hadoop,it has access to hadoop core,which helps in using mappers to slice the incoming data into 
unstructured formats and place the data in hdfs, it exports data back into rdb,ensuring that the schema of data in the database is maintained

sqoop execution process= the dataset being transferred is divided into partitions,the map only job is launched with individual mappers responsible for transferring a slice of the dataset.
,each record of data is handled in a type -safe manner

SQOOP IMPORT PROCESS= gathering metadata,job submitted to cliuster,data is transferred

APACHE FLUME : IT IS A DISTRIBUTED AND RELIABLE SERVICE FOR EFFICIENTLY COLLECTING,AGGREGATING AND MOVING LARGE AMOUNTS OF STREAMING DATA INTO THE HADOOP SYSTEM,
IT HAS A SIMPLE AND FLEXIBLE architecture which is robust and fault-tolerant,based on streaming data flows

flume model : agent,processor,collector

APACHE KAFKA= IT IS A HIGH PERFORMANCE ,REAL TIME MESSAGING system. it is an open source tool and is part of apache projects
the characteristics of kafka are; IT IS A DISTRIBUTED AND PARTITIONED MESSEGING SYSTEM,IT IS HIGHLY FAULT-TOLERANT,IT IS HIGHLY SCLAABLE,IT CAN PROCESS AND SEND MILLIONSOF MESSAGES PER SECOND TO SEVERAL RECEIVERS

kafka can be used in various use cases,messeging service,real time processing,log aggregation,commit log service,event sourcing,website activity tracking

kafka data model consist of messages and topics
messages: represents information such as lines in a log file,a row of stock market data ,or an error message.
messages are grouped into categories called topics,eg. log message and stock message

PIG = 

HIVE = IT PROVIDES SQL LIKE INTERFACE FOR USERS TO EXTRACT DATA FROM THE HADOOP SYSTEM
FEATURES OF HIVE =  USES hiveQL,high level abstraction layer on top of mapreduce and apache spark,suitable for structured data (analysis of historical data)
Hive architecture = the major component of hive architecture are hadoop core comp. ,metastore,driver, and hive clients.
Job exection flow in HIVE : RECIVE SQL QUERRY 1.parse hive QL,
2.MAKE optimization
3.plan execution.
4.submit job to cluster
5.monitor progress
6.process data in mapreduce or apache spark7.store data in hdfs

FILE FORMAT = TEXT FILE(human readable),avro ,parquet ,sequence(not readaable)

avro= efficient storage (optimized binary encoding) ,widely supported inside and outside hadoop ecosystem,
ideal for long term storage of data,can read from and write in many languages,embeds meta data in the file,
considered the best choice for general purpose storage in hadoop

parquet=  is columnar format developed by cloudera and twittter,uses advance optimization described in google dremel paper,
considered most efficient for adding multiple records at a time

HIVE OPTIMIZATION = PARTITIONING,BUCKETING & SAMPLING
non partitioned table : hive will need to read all the files in a table data directory,the process can be very slow and expensive,especialyy when the tables are large

partitions are horizontal slices of data that allow larger sets of data to be seprated in more manageable chunks
,means use partitioning to store data in separate files by (state table col in video),
a partition column is a virtual column were data is not actually stored in the file,if 50 states then 50 partition

DATA INSERTION 
= DATA INSERTION INTO PARTITIONED TABLES CAN BE DONE IN two ways or modes static and dynamic partitioning
with large amount od data stored in a table dynamic partitioning si suitable.
When not to use partitioning: when column have too many unique rows,
when creating a dynamic partition as it can lead to high number of partition,when the partition is less than 20k in size

OVER PARTITIONING CAN BECOME A PROBLEM : hERE COMES THE BUCKETING, bucketing is an optimization technique
buckets distribute the data load into user defined set of cluster by calculating the hash code of the key mentioned in the querry

NOSQL db
= IS A FORM OF UNSTRUCTURED STORAGE,WITH EXPLOSION OF SOCIAL MEDIA SITES,SUCH AS FB,TWITTER THE DEMAND TO MANAGE LARGE DATA HAS GROWMN TREMEDNDIOUSLY
NO SQL IS key value pair db,documnet db,column based,graph
KEY VALUE : ORACLE nOSQL,redis server,scalaris
document based db : mongo db,couch db,raven db
column based db : big table, cassandra,hbase,hypertable
graph based db : neo4j,infogrid,infinite graph,flock db 

HBASE : HBASE RESTS ON TOP OF HDFS AND ENALES REAL TIME ANALYSIS OF DATA,IT CAN  STORE HUGE AMOUNT OF DATA FOR EXTREAMLY FAST READS AND WRITES
hbase is mostly ised in scenario that requires regular and consistent inserting and overwriting of data
HDFS stores processes and manages large amount of data efficinetly .however ,it performs only batch processing and the data will be accessed in a sequential manner,
therefore a soltion is required to access read or write data anytime regardless of its sequence in the clusters of data

Hbase is  a type of nosql database and is classified as a key value store.
in hbase value is identifies with a key, key and value are bytearray,value are stored in key orders,can be quicly accessed by value keys

DATA STORAGE IN HDFS :data is stored in files called hfiles or storefiles that are usually saved in hdfs,
hfile a a key value map,when data is added ,it is written to a log called write ahead loag and sstored in the memory,memstore.
HFILES are immutable,since HDFSdoes not support updates to an existing file

when to use HBASE  : when you have miilions or biilions of rows,sufficient commodity h/w with at least 5 nodes,for random select and range scan by key

apace spark: next gen big data framework
SPARK RDD : spark resilient distributed dataset(rdd) is an immutable collection of objects that defines the data structure of spark
spark rdd is a fault tolerant collection of elements that can be opearted paralley

--LAZY EVALUATION : ALL transformation in apache spark are lazy they did not compute there result right away,they just remember the transformation applied to base dataset 
--Corse -grained operation : it applies to all elements in dataset to map ,filter,group by opearion
--in memory computation : it store intermediate result in distributed memory,ram instead of stable storage such as disk
--fault tolerant : as they track data lineage information,inorder to build lost data automaticlay on failure,they will rebuild lost data on failure using this lineage data,each rdd remember how it is created
from other datasets in order to recreate itself anytime 
--partitioning : fundamental unit of parallelism in spark rdd,each partiotion is 1 logical division of data which is mutable or changeable
one can create a partition through transformation on existing partition.
--persistence : users can state which rdd they will reuse and  choose a storage strategy fro each of them
--immutable : data is saved to share  across all process, they can also be created anytime which makes caching,sharing,replication easy
--location stickyness :  rdd are capable of defining placement prefrence to compute partitions. the placement prefrence refers to information about the location of each rdd

RDD SUPPORTS TWO TYPES OF OPEARION : Transformation & action
tranformation allow us to crwate new dataset using existng one,map is a transformation  that passes each dataset eelement through a function and returns a new rdd representing the results
there are two types of transformation narrow & wide transformation.
narrow : is self sufficient .it is the result of map and filter  such that  the data is from a single partition only.
function used in narrow : map,flatmap,mappartition,filter,sample,union

wide : is not self sufficient. its the result of group by key and reducebykey() like function ,such that  the data  vcan be  from multiple partitions.also known as shuffle transformation.
function used for this type of transformation: cartesianjoin,intersection,repartition,reducebykey,groupbykey,coalace,distinct

ACTIONS : ALLOW US TO RETURN TO THE DRIVER PGRM,AFTER RUNNING A COMPUTATION ON THE dataset. actions are the rdd operation that produce non rdd-values
reduce is an action that aggregates all the element of te rdd using some function
some actions : first ,take,reduce,collect,count

caching & persistance are the techniques used to store the result of rdd evaluation. they are also used by devlopers to enhance performance of applications
cost & time & less execution time
methods of caching and persistance : cache() & persist().these methods can be used to store all the rdd in memory and used them efficiently across opeartions
the diff btw cache & persist is that while using cache(),the default storage level is memory_only,whereas while using persist(),various storage levels can be used

RDD Lineage : rdd lineage is A GRAPH THAT CONTAINS THE EXISTING RDD AND THE NEW RDD CREATED FROM THE EXISTING ONE AS a result of transformation.
when a new rdd has been created from an existing rdd,the new rdd contains a pointer to the parent rdd.similarly,all the dependencies between the rdd will be logged in a grap,rather 
than the actual data.

DAG : is a graph where rdd and the opeartions to be performed oon rdd are represented in the form of vertices and edges ,respectivley

partitioning in spark : tuples in the same partition are guranteed to be on the same machine.
eaach machine contains one or more partitions,& the total no. of partitions is configureable
there are types of partition in spark 
hash : if we did not mention any partitioner,then spark will use this hash partitioner for partitioning the data
range :if there are sortable records,then range partition will divide the records alomost in equal ranges
custom partitioning :  we can customize the number of partitions we need and what should be stored in those partitions.
partitions can be done based on size,partition can be doe by specifying the minimum no. of  partitions as textfile(file,minpartitions)
,while running on a cluster the minimum no. of partitions by default is 2, more partitions=more parallelization

RDD OPERATION work on each of the following partitions:
foreachpartition : call a function foreach
mappartition : will create a new rdd by executing a function on each partition on the current rdd
mappartitionwithindex : same as above ,but does include index of partition function

SHUFFLING : ITS AN OPERATION which require one node that will fetch data from other nodes to have data for computing the result.
,fetching data via network results in data partitioning sorting,serialization,disk and network i/o which are expensive to the applications

hash shuffle: each task will write the output into multiple files (this is ast,no i/o overhead, no memory overhead)
sort shuffle : each task spills only one shuffle containig segments and one index file

Unsafe shuffle : the reccords  are serialized once and then stored in memory pages.
it has single serialization,no deserialization/serialization for merging spill files,and sorting is cpu cache friendly

Aggregation : to perform aggregation ,the datasets must be described in the form of key/value pairs,key/value pairs are a common data type required for many operation in spark
functions that can be used for aggregation : reducebykey(),foldbykey(),mapvalues()

SPARK SQL -PROCESSING FRAMEWORK
ADVANTAGE OF SPARK SQL
integrated:mixes sql queries with spark programs
unified data access: loads and queries data from differet sources
standard connectivity:connects through jdbc or odbc
performance :executes the most optimal plans

SQL CONTEXT :THE SQL context class or any of its descendants acts as the entry point into all functionalities
q.how to get the benefit of a superset of the basic sql context functionality?
build a hive context to:
use the writing ability for queries
access hive udf and read data from hive tables
--we can use the spark.sql.dialect option to select the specific variant of sql used for parsing queries.
--on a sql context ,the sql function allows application to programmatically run sql queries and then return a dataframe as the result
eg. val df=sqlcontext.sql("SELECT * FROM table")

DATAFRAMES : represents a distributed collection of data is organised into columns that are named
--construct a dataframe:use sources such as tables in hive ,structured data files,existing RDD,and external databases
--convert them into RDDs : call the rdd method that returns the dataframe  contents as an RDD of rows
in prior version of spark sql api,scehemaRDD has been renamed as dataaframe

USE CASE : RDD API 
we can use any three api based on requirements
RDD : LOW LEVEL TRANSFORMATION & UNSTRUCTURED DATA
USE CASE : DATAFRAME & DATASET API
IN SPAark dataframe api and datasets api were merged.unifying data processing capabilities across libraries
dataframe & dataset : high evel abstraction,unifies access,high level expression(filter ,map,aggregation,avrg,sum,sql queries,use of lambda funnction on semi structured data),
high degree of typesafety at compile time
 
 DATAFRAME USED MOSTLY ,NO USE OF RDD NOW
 
Nosql:nosql db are commonly used to solve challenges posed by stream processing techniques.
,minimal latency,high storage,scalability

REAL TIME PROCESSING of big data : processing of continuous input,processing and analysis of reporting data
-the process consists of a sequence of repeated operations.in which the data streams are transferred  to the memory
-real time processing is crucial in order to continue the high level functionality of automated system having intensive data streams and different  data structures
-ex bank atm,radar system,disaster mgmt,iot,social n/w

Data ingestion : flume,kafka
data storagestream processing: spark streaming,storm,s4
analytical data store : Hbase,HIVE,cassandra
Analysis & reporting :splunk,sap hana

data processing architecture for real time processing should have the following properties
fault tolerant & scalabel ,supportive of batch and incremental updates,extensible

The Lambda architecture :it composed of threee layers batch,real time,serving
batch layer
-store the raw data as it arrives
-compute the batch views for consumption
-manages historical data 
-recompute results such as machine learning models
-operates on full data
-produces most accurate results
-has high cost of high latency due to high computation time

Real time layer
-receives the arriving data
-performs incremental updates to the batch layer results
-has incremental algorithims implemented at the speed layer
-has a significantly reduced computtional cost
 
The Kappa architecture : only processes data as stream
-handle both real time data processing and continuous data reprocessing using a single stream processing engine
-require incoming data stream to be replayed very quickly
-in case of code changes ,a second stream process replays all previous data and replaces the data in the serving layer
-simplyfy by only keeping one code base
